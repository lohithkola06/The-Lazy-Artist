<PRD>

# The Lazy Artist: Interpretable Computer Vision & Spurious Correlation Analysis

## 1. Introduction

### 1.1 Document purpose

This Product Requirements Document (PRD) defines the specifications for "The Lazy Artist," a computer vision research project that investigates how neural networks learn and exploit spurious correlations in image classification tasks. The document serves as a comprehensive guide for developers, researchers, and stakeholders to understand the project scope, technical requirements, and expected deliverables.

### 1.2 Document scope

This PRD covers the complete lifecycle of the project, from dataset generation through model training, interpretability analysis, and robust training interventions. It includes both core requirements (Tasks 0-4) and optional bonus features (Tasks 5-6).

### 1.3 Project context

The project explores a fundamental problem in machine learning: models often learn shortcuts based on spurious features rather than true causal relationships. Using a Colored-MNIST dataset where digits are artificially correlated with colors, the project demonstrates how models become "lazy artists"—relying on color cues rather than learning robust shape representations. Through interpretability techniques and training interventions, the project aims to understand and mitigate this behavior.

---

## 2. Product overview

### 2.1 Product description

"The Lazy Artist" is a comprehensive research implementation that combines dataset engineering, deep learning, and interpretability methods to study spurious correlation learning in computer vision. The system generates a biased training dataset, demonstrates model failure on distribution-shifted test data, provides tools for neural network interpretation, and implements training strategies to build more robust models.

### 2.2 Key capabilities

- **Biased dataset generation**: Create Colored-MNIST datasets with controlled spurious correlations
- **Baseline model training**: Train and evaluate standard CNNs that exploit spurious features
- **Neural interpretation**: Implement activation maximization and Grad-CAM from scratch
- **Robust training**: Develop custom training strategies to reduce spurious correlation reliance
- **Adversarial analysis**: (Bonus) Generate targeted adversarial examples to test model robustness
- **Feature decomposition**: (Bonus) Train sparse autoencoders for mechanistic interpretability

### 2.3 Success criteria

- Baseline model achieves >95% accuracy on Easy split but <20% on Hard split
- Robust model achieves >70% accuracy on Hard split without dataset modifications
- All experiments are reproducible from documented notebooks
- Clear interpretability evidence demonstrates what features models learn
- Comprehensive report documents methodology, results, and insights

---

## 3. Goals and objectives

### 3.1 Primary goals

1. **Demonstrate spurious correlation vulnerability**: Show quantitatively how standard training procedures lead to models that exploit dataset biases
2. **Build interpretability tools**: Implement from-scratch methods to visualize and understand what neural networks learn
3. **Develop mitigation strategies**: Design and validate training interventions that improve robustness without changing the input data
4. **Ensure reproducibility**: Create a well-documented, executable research artifact that others can build upon

### 3.2 Learning objectives

- Understand the mechanics of spurious correlation learning in deep networks
- Gain hands-on experience with neural network interpretability techniques
- Develop skills in designing robust training objectives and regularization strategies
- Practice rigorous experiment design, logging, and reporting

### 3.3 Research questions

- Which neurons/channels in a biased model encode color vs. shape information?
- Where does a biased model focus attention (color or shape) when both cues are present?
- What training interventions can encourage shape-based rather than color-based classification?
- How does robustness to distributional shift relate to robustness to adversarial perturbations?

---

## 4. Target audience

### 4.1 Primary users

- **Machine learning researchers**: Exploring interpretability, robustness, and fairness in neural networks
- **Computer vision students**: Learning practical skills in deep learning, debugging, and model analysis
- **ML practitioners**: Seeking methods to diagnose and mitigate spurious correlation issues in production systems

### 4.2 Secondary users

- **Educators**: Using the project as a teaching tool for AI safety and robustness concepts
- **Reviewers/evaluators**: Assessing the technical implementation and experimental rigor
- **Open-source contributors**: Extending the codebase with additional interpretability methods or datasets

### 4.3 User expertise assumptions

- Intermediate Python programming skills
- Familiarity with PyTorch or similar deep learning frameworks
- Basic understanding of convolutional neural networks
- Comfort with Jupyter notebooks and version control (Git)

---

## 5. Features and requirements

### 5.1 Core features (Tasks 0-4)

#### 5.1.1 Dataset generation engine
- Colored-MNIST creation with configurable correlation strengths
- Split-specific color assignments (Easy vs. Hard)
- Foreground colorization with texture support
- Metadata tracking and versioning

#### 5.1.2 Baseline training pipeline
- CNN architecture implementation (ResNet-18 or custom)
- Standard supervised training loop
- Performance evaluation across multiple splits
- Confusion matrix and failure case analysis

#### 5.1.3 Interpretability toolkit
- Activation maximization for neuron probing
- From-scratch Grad-CAM implementation
- Visualization utilities for heatmaps and artifacts
- Comparative analysis tools for biased vs. conflicting samples

#### 5.1.4 Robust training system
- Custom loss functions and regularizers
- Multiple training strategy implementations
- Hyperparameter management
- Comparative evaluation framework

### 5.2 Bonus features (Tasks 5-6)

#### 5.2.1 Adversarial attack module
- Targeted attack implementation (7→3)
- Epsilon-bounded perturbation generation
- Robustness comparison metrics
- Attack transferability analysis

#### 5.2.2 Sparse autoencoder system
- Overcomplete feature learning
- Manual feature inspection interface
- Feature intervention capabilities
- Downstream prediction tracking

### 5.3 Cross-cutting features

#### 5.3.1 Experiment management
- Comprehensive logging of all hyperparameters
- Deterministic seeding for reproducibility
- Training curve tracking (loss, accuracy)
- Checkpoint saving and loading

#### 5.3.2 Documentation and reporting
- Executable Jupyter notebooks with narrative
- Auto-generated metric summaries
- Visualization export utilities
- Final report/presentation generation

---

## 6. User stories and acceptance criteria

### 6.1 Dataset generation (Task 0)

**ST-101: Generate biased Colored-MNIST dataset**
- **As a** researcher
- **I want to** generate a Colored-MNIST dataset with controlled spurious correlations
- **So that** I can train models on biased data and test on distribution-shifted data

**Acceptance criteria:**
- System generates train/val/test splits with configurable sizes
- Each digit class (0-9) has an assigned dominant color
- Easy split enforces 95% digit↔color correlation, 5% random counter-examples
- Hard split inverts correlations (dominant color never appears with its digit)
- Color is applied to foreground strokes or as background texture (not flat background)
- Generated dataset includes metadata JSON with color mapping, correlation strengths, and seed
- Verification script confirms correlation percentages match specifications

**ST-102: Visualize dataset samples**
- **As a** researcher
- **I want to** visualize sample images from each split
- **So that** I can verify the colorization quality and correlation structure

**Acceptance criteria:**
- System displays grid of sample images from Easy and Hard splits
- Visualization shows digit label, applied color, and split type
- Counter-examples (5% random colors in Easy split) are identifiable
- Notebook cell includes both biased and conflicting samples

**ST-103: Load and preprocess colored dataset**
- **As a** model trainer
- **I want to** load the colored dataset with standard PyTorch data loaders
- **So that** I can train models efficiently

**Acceptance criteria:**
- DataLoader supports batching, shuffling, and parallel loading
- Images are normalized to [0, 1] or [-1, 1] range
- Labels are correctly associated with images
- Dataset object reports split name and size

---

### 6.2 Baseline training (Task 1)

**ST-201: Train baseline CNN on Easy split**
- **As a** researcher
- **I want to** train a standard CNN on the biased Easy training data
- **So that** I can establish a baseline that exploits spurious correlations

**Acceptance criteria:**
- Model architecture is a ResNet-18 or equivalent small CNN (documented in code)
- Training runs for sufficient epochs to converge (e.g., 20-50 epochs)
- Training uses standard cross-entropy loss and Adam/SGD optimizer
- Model achieves >95% accuracy on Easy train and Easy validation sets
- Training curves (loss, accuracy) are logged and plotted
- Final model checkpoint is saved with metadata (epoch, accuracy)

**ST-202: Evaluate baseline on Hard test split**
- **As a** researcher
- **I want to** evaluate the baseline model on the Hard test split
- **So that** I can quantify performance degradation due to distribution shift

**Acceptance criteria:**
- System loads trained baseline model and evaluates on Hard test set
- Test accuracy is computed and logged (expected <20%)
- Per-class accuracies are reported
- System identifies which digit classes suffer most from distribution shift
- Results are compared against Easy test accuracy

**ST-203: Generate confusion matrix for Hard test**
- **As a** researcher
- **I want to** see a confusion matrix for Hard test predictions
- **So that** I can understand which digit pairs are confused

**Acceptance criteria:**
- System generates and displays a 10×10 confusion matrix
- Matrix is normalized to show percentages per true class
- Visualization uses appropriate color scale (e.g., heatmap)
- Dominant off-diagonal patterns are highlighted in analysis

**ST-204: Demonstrate color reliance with counterfactuals**
- **As a** researcher
- **I want to** artificially recolor test images and observe prediction changes
- **So that** I can prove the model relies on color rather than shape

**Acceptance criteria:**
- System takes a correctly classified Easy image (e.g., red "0")
- System recolors it to another digit's dominant color (e.g., green "1")
- Model's new prediction matches the new color's associated digit
- At least 3 counterfactual examples are documented with before/after predictions
- Notebook includes visual comparison of original vs. recolored images

---

### 6.3 Neuron probing (Task 2)

**ST-301: Implement activation maximization**
- **As a** interpretability researcher
- **I want to** optimize input images to maximize specific neuron activations
- **So that** I can understand what features individual neurons detect

**Acceptance criteria:**
- System accepts model, layer name, and neuron index as inputs
- Optimization initializes a random input tensor with gradient enabled
- Loss function maximizes target neuron activation (raw, channel mean, or spatial mean)
- Optimizer runs for configurable iterations (e.g., 500-1000)
- Regularization (e.g., L2 penalty, blur, jitter) prevents noisy artifacts
- Output image is normalized and saved with neuron identifier

**ST-302: Probe multiple neurons across layers**
- **As a** researcher
- **I want to** generate activation maximization visualizations for multiple neurons
- **So that** I can compare what different layers and units learn

**Acceptance criteria:**
- System probes at least 5 neurons from early, middle, and late convolutional layers
- Each probe generates a distinct visualization
- Visualizations are organized in a grid by layer and neuron index
- System supports batch probing (e.g., probe all channels in a layer)

**ST-303: Analyze qualitative patterns in probed features**
- **As a** researcher
- **I want to** document qualitative observations from neuron visualizations
- **So that** I can identify color-sensitive vs. shape-sensitive neurons

**Acceptance criteria:**
- Notebook includes annotated observations for each probed neuron
- At least one neuron is identified as "color-sensitive" (responds to uniform color)
- At least one neuron is identified as "shape-sensitive" (responds to edges/textures)
- Polysemantic neurons (responding to multiple features) are noted if present
- Summary table categorizes neurons by their apparent function

---

### 6.4 Grad-CAM interpretability (Task 3)

**ST-401: Implement Grad-CAM from scratch**
- **As a** developer
- **I want to** build a from-scratch Grad-CAM implementation
- **So that** I avoid using pre-built libraries and understand the internals

**Acceptance criteria:**
- Code does NOT import `pytorch-gradcam` or equivalent libraries
- Implementation hooks the final convolutional layer to capture feature maps
- Forward pass computes target class score
- Backward pass computes gradients of score w.r.t. feature maps
- Gradients are globally averaged to get channel importance weights
- Weighted sum of feature maps produces raw heatmap
- Heatmap is ReLU'd (negative values zeroed) and upsampled to input size

**ST-402: Generate Grad-CAM visualizations**
- **As a** researcher
- **I want to** overlay Grad-CAM heatmaps on input images
- **So that** I can see where the model focuses attention

**Acceptance criteria:**
- System generates heatmap for a given input image and target class
- Heatmap is colorized (e.g., jet colormap) and alpha-blended with original image
- Output image clearly shows attention regions
- System exports visualization as PNG with filename indicating sample ID and class

**ST-403: Compare Grad-CAM on biased vs. conflicting samples**
- **As a** researcher
- **I want to** generate Grad-CAM for both biased and conflicting color-digit pairs
- **So that** I can determine whether attention focuses on color or shape

**Acceptance criteria:**
- System selects a biased sample (e.g., red "0" correctly predicted as "0")
- System selects a conflicting sample (e.g., green "0" misclassified as "1")
- Grad-CAM is generated for both samples targeting their predicted classes
- Side-by-side comparison shows attention on color regions for biased model
- Notebook includes written analysis of where attention concentrates

**ST-404: Analyze Grad-CAM patterns across multiple samples**
- **As a** researcher
- **I want to** generate Grad-CAM for a batch of test images
- **So that** I can identify systematic attention patterns

**Acceptance criteria:**
- System processes at least 10 Hard test images
- Grad-CAM visualizations are saved for all images
- Qualitative analysis notes whether attention focuses on digit strokes or background color
- Summary quantifies proportion of samples with color-focused vs. shape-focused attention

---

### 6.5 Robust training interventions (Task 4)

**ST-501: Design custom robust training strategy**
- **As a** ML engineer
- **I want to** implement custom training strategies to reduce color reliance
- **So that** models learn shape-based representations without converting inputs to grayscale

**Acceptance criteria:**
- At least two distinct training strategies are implemented (e.g., gradient penalties, saliency-guided regularization, invariance losses)
- Each strategy is documented with theoretical justification
- Strategies do NOT convert images to grayscale or modify the dataset
- Strategies are modular and can be toggled via configuration flags
- Code includes comments explaining each component

**ST-502: Train robust model on Easy split**
- **As a** researcher
- **I want to** train a model using robust training strategies
- **So that** it learns shape-based features despite biased training data

**Acceptance criteria:**
- Model is trained on the same Easy training split as baseline
- Training uses the same architecture for fair comparison
- Custom loss functions or regularizers are applied during training
- Training converges to >90% accuracy on Easy validation (may be slightly lower than baseline)
- All hyperparameters (learning rate, regularization weights) are logged

**ST-503: Evaluate robust model on Hard test split**
- **As a** researcher
- **I want to** evaluate the robust model on Hard test data
- **So that** I can measure improvement over baseline

**Acceptance criteria:**
- Robust model achieves >70% accuracy on Hard test split
- Improvement over baseline is quantified (e.g., 70% vs. 15% = +55 percentage points)
- Per-class accuracies are compared between baseline and robust models
- Confusion matrix for robust model shows more diagonal concentration

**ST-504: Compare baseline vs. robust models**
- **As a** researcher
- **I want to** see side-by-side comparisons of baseline and robust models
- **So that** I can understand which interventions were effective

**Acceptance criteria:**
- Comparison table includes: Easy train acc, Easy val acc, Easy test acc, Hard test acc
- Grad-CAM visualizations are generated for robust model on same conflicting samples
- Robust model's Grad-CAM shows attention shift toward shape rather than color
- Counterfactual recoloring tests show robust model is less affected by color changes
- Notebook includes discussion of which training strategy components were most effective

**ST-505: Analyze failure cases of robust model**
- **As a** researcher
- **I want to** inspect failure cases of the robust model on Hard test
- **So that** I can identify limitations and areas for improvement

**Acceptance criteria:**
- System identifies at least 5 incorrectly classified Hard test samples
- Each failure case includes: image, true label, predicted label, prediction confidence
- Qualitative analysis discusses why these samples are difficult
- Grad-CAM for failure cases shows where model focused attention

---

### 6.6 Adversarial attacks (Task 5 - Bonus)

**ST-601: Implement targeted adversarial attack**
- **As a** security researcher
- **I want to** generate adversarial examples that fool the model
- **So that** I can test model robustness to imperceptible perturbations

**Acceptance criteria:**
- System implements a targeted attack (e.g., PGD, C&W)
- Attack takes a source image (digit "7") and target class ("3")
- Optimization minimizes loss that encourages target class prediction
- Perturbation is clipped to epsilon < 0.05 per pixel
- Attack runs for configurable iterations until >90% confidence on target class
- Perturbed image is visually similar to original (verified manually)

**ST-602: Compare attack success on baseline vs. robust models**
- **As a** researcher
- **I want to** measure attack difficulty for baseline and robust models
- **So that** I can quantify whether robust training improves adversarial robustness

**Acceptance criteria:**
- Same attack is applied to both baseline and robust models
- For each model, system records: attack success rate, average perturbation magnitude, iterations to success
- Results show whether robust model requires larger perturbations or more iterations
- At least 10 adversarial examples are generated for each model
- Comparison table summarizes attack effectiveness metrics

**ST-603: Visualize adversarial perturbations**
- **As a** researcher
- **I want to** see the perturbation patterns added to images
- **So that** I can understand what the attack exploits

**Acceptance criteria:**
- System visualizes perturbation (difference between original and adversarial image)
- Perturbation is amplified (e.g., ×10) for visibility
- Side-by-side comparison shows: original image, perturbation, adversarial image
- Model predictions and confidences are displayed for original and adversarial images

---

### 6.7 Sparse autoencoder decomposition (Task 6 - Bonus)

**ST-701: Train sparse autoencoder on activations**
- **As a** mechanistic interpretability researcher
- **I want to** train a sparse autoencoder on hidden layer activations
- **So that** I can decompose representations into interpretable features

**Acceptance criteria:**
- SAE is trained on cached activations from a chosen hidden layer (e.g., penultimate layer)
- Encoder expands to overcomplete dimension (e.g., 512 → 2048)
- Loss includes reconstruction term and sparsity penalty (L1 on latents)
- Training converges to low reconstruction error with high sparsity
- Trained SAE encoder and decoder are saved

**ST-702: Inspect and label learned features**
- **As a** researcher
- **I want to** visualize what each SAE feature detects
- **So that** I can manually label them as color/shape/polysemantic features

**Acceptance criteria:**
- System generates activation maximization or top-activating dataset examples for each SAE feature
- At least 20 features are inspected and labeled
- Labels include categories: "color-sensitive," "shape-sensitive," "polysemantic," "unclear"
- Feature labels are stored in a CSV or JSON file

**ST-703: Perform feature interventions**
- **As a** researcher
- **I want to** scale individual SAE features up/down and observe prediction changes
- **So that** I can test causal effects of features on model behavior

**Acceptance criteria:**
- System allows setting a feature's activation to 0× (ablate), 2× (amplify), or custom scalar
- Intervened activation is decoded back to original space and passed through rest of model
- System records prediction changes for each intervention
- At least 5 features are tested with ablation and amplification
- Results show how color features vs. shape features affect predictions differently

---

### 6.8 Experiment management and reproducibility

**ST-801: Set global random seed**
- **As a** researcher
- **I want to** set a global seed at the start of each experiment
- **So that** all results are deterministic and reproducible

**Acceptance criteria:**
- Notebook includes a configuration cell with a `SEED` variable
- Seed is set for Python `random`, NumPy `np.random`, and PyTorch `torch.manual_seed`
- Seed is applied to DataLoader workers (via `worker_init_fn`)
- Same seed produces identical training curves and final metrics across runs

**ST-802: Log all hyperparameters**
- **As a** researcher
- **I want to** automatically log all hyperparameters for each experiment
- **So that** results can be replicated and compared

**Acceptance criteria:**
- Config dictionary includes: model architecture, learning rate, batch size, epochs, regularization weights, dataset split ratios, seed
- Config is serialized to JSON and saved alongside checkpoints
- Notebook cells display config at the start of each experiment
- Logs include timestamp and experiment name

**ST-803: Track and visualize training curves**
- **As a** researcher
- **I want to** see training and validation metrics over time
- **So that** I can diagnose overfitting or convergence issues

**Acceptance criteria:**
- System logs loss and accuracy for train and validation sets each epoch
- Metrics are plotted in real-time or at the end of training
- Plots include legends and axis labels
- Training curves are saved as PNG files

**ST-804: Save model checkpoints**
- **As a** researcher
- **I want to** save trained models with metadata
- **So that** I can reload them for evaluation or further experiments

**Acceptance criteria:**
- Checkpoint includes model state_dict, optimizer state_dict, epoch, metrics
- Checkpoint filename includes experiment name and timestamp
- System supports loading checkpoint and resuming training
- Best model (by validation accuracy) is saved separately

---

### 6.9 Documentation and reporting

**ST-901: Maintain clear README**
- **As a** external user
- **I want to** understand the project structure and how to run it
- **So that** I can reproduce results without confusion

**Acceptance criteria:**
- README includes: project description, directory structure, installation instructions, commands to run experiments
- Installation instructions list required packages (e.g., `requirements.txt` or `environment.yml`)
- Commands to reproduce each task are documented (e.g., "Run `notebooks/task1_baseline.ipynb`")
- README mentions Python version and hardware requirements

**ST-902: Write comprehensive Jupyter notebooks**
- **As a** researcher
- **I want to** include narrative explanations alongside code
- **So that** the notebook reads like a technical report

**Acceptance criteria:**
- Each notebook corresponds to a major task (Task 0-6)
- Notebooks include markdown cells explaining methodology, hypotheses, and results
- Code cells are well-commented and modular
- Visualizations are embedded inline with interpretations
- Notebooks can be executed top-to-bottom without errors

**ST-903: Generate final report/presentation**
- **As a** project stakeholder
- **I want to** read a summary of methodology, experiments, and findings
- **So that** I can understand the project without running code

**Acceptance criteria:**
- Report is a PDF or Markdown document (5-10 pages)
- First page states what was done and what was not done (and why)
- Report includes: introduction, dataset description, baseline results, interpretability findings, robust training results, conclusions
- Report contains tables, plots, and example images
- Negative results and failure cases are discussed openly
- Report suggests future work directions

**ST-904: Justify design choices**
- **As a** reviewer
- **I want to** understand why specific methods were chosen
- **So that** I can assess the rigor of the research

**Acceptance criteria:**
- Notebooks and report explain: why chosen CNN architecture, why chosen robust training strategies, why chosen hyperparameters
- Ablation studies or pilot experiments are referenced where applicable
- Limitations of chosen methods are acknowledged
- Alternative approaches are discussed briefly

---

## 7. Technical requirements / Stack

### 7.1 Programming language and environment

- **Python 3.8+**: Primary language for all implementations
- **Jupyter Notebook**: For interactive experimentation and documentation
- **Git**: Version control with GitHub repository (public or private)

### 7.2 Core libraries and frameworks

#### 7.2.1 Deep learning
- **PyTorch 1.10+**: Neural network training and inference
- **torchvision**: Pre-trained models (ResNet-18) and transforms
- **torch.optim**: Optimizers (Adam, SGD)
- **torch.nn**: Loss functions and layers

#### 7.2.2 Data handling
- **NumPy**: Array operations and dataset generation
- **PIL/Pillow**: Image loading and manipulation
- **torchvision.datasets**: Base MNIST dataset

#### 7.2.3 Visualization
- **Matplotlib**: Plotting training curves and confusion matrices
- **seaborn**: Enhanced visualization aesthetics
- **OpenCV (optional)**: Advanced image processing

#### 7.2.4 Experiment management
- **JSON**: Configuration and metadata serialization
- **pickle**: Checkpoint saving
- **tqdm**: Progress bars for long-running operations

### 7.3 Hardware requirements

#### 7.3.1 Minimum requirements
- CPU: Multi-core processor (4+ cores recommended)
- RAM: 8 GB minimum
- Storage: 2 GB for datasets, checkpoints, and results

#### 7.3.2 Recommended requirements
- GPU: NVIDIA GPU with 4+ GB VRAM (CUDA support)
- RAM: 16 GB for larger batch sizes and caching
- Storage: 10 GB for extensive experiment logging

### 7.4 Dataset specifications

- **Base dataset**: MNIST or Fashion-MNIST (28×28 grayscale)
- **Colorized format**: RGB images (28×28×3)
- **Training set**: ~50,000 samples (Easy split with 95% correlation)
- **Validation set**: ~10,000 samples (Easy split)
- **Test set**: ~10,000 samples (Hard split with inverted correlations)
- **Storage format**: PyTorch tensor files or HDF5 for efficient loading

### 7.5 Model architecture specifications

#### 7.5.1 Baseline CNN
- **Option 1**: ResNet-18 with modified first layer (3→3 channel input)
- **Option 2**: Custom CNN with 3-4 convolutional blocks
- **Input size**: 28×28×3 RGB images
- **Output**: 10-class logits (digits 0-9)
- **Activation**: ReLU
- **Normalization**: Batch normalization

#### 7.5.2 Sparse autoencoder (bonus)
- **Input dimension**: Hidden layer size (e.g., 512)
- **Latent dimension**: Overcomplete (e.g., 2048)
- **Architecture**: Single hidden layer encoder/decoder
- **Activation**: ReLU for encoder
- **Sparsity**: L1 penalty on latent activations

### 7.6 Training specifications

- **Batch size**: 128-256 (adjustable based on GPU memory)
- **Epochs**: 20-50 for convergence
- **Optimizer**: Adam (lr=0.001) or SGD with momentum
- **Loss**: Cross-entropy for classification
- **Evaluation frequency**: Every epoch
- **Checkpointing**: Save best model by validation accuracy

### 7.7 Interpretability method specifications

#### 7.7.1 Activation maximization
- **Optimization steps**: 500-1000 iterations
- **Optimizer**: Adam (lr=0.1)
- **Regularization**: L2 norm penalty, Gaussian blur every N steps
- **Input initialization**: Random noise or gray image

#### 7.7.2 Grad-CAM
- **Target layer**: Final convolutional layer (before global pooling)
- **Upsampling**: Bilinear interpolation to input size
- **Colormap**: Jet or Viridis for heatmap visualization
- **Alpha blending**: 0.4-0.6 for overlay transparency

### 7.8 Reproducibility requirements

- **Deterministic operations**: Set `torch.backends.cudnn.deterministic = True`
- **Seed propagation**: All random operations (data augmentation, dropout) seeded
- **Version pinning**: `requirements.txt` with exact package versions
- **Environment specification**: Conda `environment.yml` or Docker container (optional)

---

## 8. Design and user interface

### 8.1 Repository structure

```
lazy-artist/
├── README.md
├── requirements.txt
├── environment.yml (optional)
├── notebooks/
│   ├── task0_dataset_generation.ipynb
│   ├── task1_baseline_training.ipynb
│   ├── task2_neuron_probing.ipynb
│   ├── task3_gradcam.ipynb
│   ├── task4_robust_training.ipynb
│   ├── task5_adversarial_attacks.ipynb (bonus)
│   └── task6_sparse_autoencoder.ipynb (bonus)
├── src/
│   ├── __init__.py
│   ├── dataset.py          # Colored-MNIST generation
│   ├── models.py           # CNN architectures
│   ├── training.py         # Training loops and losses
│   ├── interpretability.py # Activation maximization, Grad-CAM
│   ├── attacks.py          # Adversarial attack implementations (bonus)
│   └── sparse_ae.py        # Sparse autoencoder (bonus)
├── data/
│   ├── colored_mnist/
│   │   ├── train/
│   │   ├── val/
│   │   ├── test/
│   │   └── metadata.json
├── checkpoints/
│   ├── baseline_model.pth
│   └── robust_model.pth
├── results/
│   ├── figures/
│   ├── logs/
│   └── report.pdf
└── tests/
    └── test_dataset.py
```

### 8.2 Notebook design principles

#### 8.2.1 Organization
- Each notebook is self-contained with clear sections: Setup, Data Loading, Model Definition, Training, Evaluation, Visualization
- Markdown headers follow hierarchical structure (# Task X, ## Step Y, ### Analysis)
- Code cells are short (<30 lines) and focused on single operations

#### 8.2.2 Narrative flow
- Notebooks read as technical reports with inline explanations
- Hypotheses are stated before experiments
- Results are interpreted immediately after generation
- Failure cases and negative results are discussed transparently

#### 8.2.3 Visualization standards
- All plots include titles, axis labels, and legends
- Consistent color schemes across experiments (e.g., blue for baseline, orange for robust)
- Figure sizes are optimized for readability (10-12 inches wide)
- Visualizations are saved to `results/figures/` with descriptive filenames

### 8.3 Code modularity

#### 8.3.1 Reusable utilities
- `src/dataset.py` provides `ColoredMNIST` class with customizable correlation parameters
- `src/models.py` exports model builder functions (e.g., `build_resnet18()`)
- `src/interpretability.py` contains reusable functions: `activation_maximization()`, `gradcam()`
- Notebooks import from `src/` to avoid code duplication

#### 8.3.2 Configuration management
- Each notebook starts with a config dictionary defining all hyperparameters
- Config is serialized and saved with results for reproducibility
- Config can be overridden via command-line arguments if notebooks are converted to scripts

### 8.4 Visualization examples

#### 8.4.1 Dataset samples
- Grid layout: 10×10 showing one example per digit class
- Each subplot annotated with true label and applied color
- Separate grids for Easy and Hard splits

#### 8.4.2 Training curves
- Line plots with train and validation accuracy/loss over epochs
- Shaded regions for standard deviation across multiple runs (if applicable)
- Vertical line marking best checkpoint epoch

#### 8.4.3 Confusion matrices
- Heatmap with true labels on Y-axis, predicted labels on X-axis
- Diagonal highlighted to emphasize correct predictions
- Off-diagonal patterns analyzed in caption

#### 8.4.4 Grad-CAM overlays
- Three-panel layout: original image, heatmap, overlay
- Color bar indicating attention intensity
- Title includes predicted class and confidence

#### 8.4.5 Activation maximization gallery
- Grid of synthesized images, one per probed neuron
- Each subplot labeled with layer name and neuron index
- Qualitative annotations ("color-sensitive," "edge detector," etc.)

### 8.5 Reporting format

#### 8.5.1 Final report structure
1. **Title page**: Project name, authors, date
2. **Executive summary** (1 paragraph): What was done and key findings
3. **Introduction**: Problem statement, motivation, research questions
4. **Methodology**: Dataset generation, baseline training, interpretability methods, robust training strategies
5. **Experiments and results**: Quantitative metrics, qualitative analysis, visualizations
6. **Discussion**: Insights, limitations, implications for robustness research
7. **Conclusion**: Summary and future work
8. **Appendix**: Hyperparameters, additional visualizations, code snippets

#### 8.5.2 Presentation format (optional)
- 10-15 slides covering: problem, approach, results, insights
- Emphasis on visual storytelling (plots, Grad-CAM comparisons, counterfactuals)
- Live demo or video recording of notebook execution

### 8.6 User interaction patterns

Since this is a research project rather than a production application, "user interaction" primarily involves:

#### 8.6.1 Running experiments
- User executes notebook cells sequentially
- Progress bars provide feedback for long-running operations (training, activation maximization)
- Intermediate results are displayed inline (e.g., "Epoch 10/50: Train Acc = 92.3%")

#### 8.6.2 Inspecting results
- User scrolls through visualizations and metrics in notebook outputs
- User can modify config cells and re-run experiments with different hyperparameters
- User exports final plots and tables to `results/` directory

#### 8.6.3 Debugging and exploration
- User can insert additional cells to inspect model weights, activations, or gradients
- User can modify interpretability methods to probe different layers or neurons
- User can add custom robust training strategies by editing `src/training.py`

### 8.7 Accessibility and usability

- **Code readability**: Follow PEP 8 style guide, use descriptive variable names
- **Documentation**: Docstrings for all functions in `src/` modules
- **Error handling**: Informative error messages for common issues (missing files, dimension mismatches)
- **Portability**: Avoid hard-coded paths, use relative paths and `os.path.join()`

---

## 9. Acceptance and delivery criteria

### 9.1 Minimum viable deliverable (Tasks 0-4)

- GitHub repository is public/private and accessible
- README documents setup and reproduction steps
- Notebooks for Tasks 0-4 execute without errors
- Baseline model achieves >95% accuracy on Easy, <20% on Hard
- Robust model achieves >70% accuracy on Hard
- Grad-CAM implementation is from scratch (no external libraries)
- Final report (PDF or Markdown) summarizes all experiments

### 9.2 Bonus deliverables (Tasks 5-6)

- Adversarial attack generates targeted perturbations with epsilon < 0.05
- Sparse autoencoder decomposes activations into interpretable features
- Feature interventions demonstrate causal effects on predictions

### 9.3 Quality standards

- All experiments are reproducible via documented random seeds
- Code passes basic linting checks (no syntax errors)
- Visualizations are publication-quality (high DPI, clear labels)
- Report discusses negative results and limitations transparently

### 9.4 Timeline and milestones

- **Week 1**: Tasks 0-1 (dataset generation, baseline training)
- **Week 2**: Tasks 2-3 (neuron probing, Grad-CAM)
- **Week 3**: Task 4 (robust training interventions)
- **Week 4**: Bonus tasks (5-6), final report, polish

---

## 10. Constraints and assumptions

### 10.1 Constraints

- Cannot convert images to grayscale for robust training (must work with color)
- Cannot modify the dataset distribution (must train on Easy split)
- Must implement Grad-CAM from scratch (no external libraries)
- Perturbation epsilon for adversarial attacks must be < 0.05

### 10.2 Assumptions

- Users have access to GPU for faster training (CPU-only is possible but slower)
- Users are familiar with Jupyter notebooks and PyTorch
- MNIST dataset is available via torchvision or can be downloaded automatically
- Training converges within 50 epochs for both baseline and robust models

### 10.3 Risks and mitigations

| Risk | Impact | Mitigation |
|------|--------|-----------|
| Robust model fails to reach >70% Hard accuracy | Project incomplete | Experiment with multiple training strategies, adjust hyperparameters, seek literature guidance |
| Grad-CAM implementation is buggy | Incorrect interpretations | Validate against known examples (e.g., ImageNet samples), compare with `pytorch-gradcam` as reference |
| Dataset generation produces flat color backgrounds | Violates FR-1.4 | Implement texture-based colorization or stroke-level color application |
| Training takes too long without GPU | Delayed timeline | Use smaller model or reduce dataset size for prototyping |
| Notebooks become too long and unreadable | Poor documentation | Split into multiple notebooks per task, use helper functions in `src/` |

---

## 11. Glossary

- **Spurious correlation**: A statistical relationship between features that does not reflect true causal structure (e.g., digit-color correlation in biased dataset)
- **Distribution shift**: A change in the data distribution between training and test sets (e.g., Easy vs. Hard splits)
- **Activation maximization**: An interpretability technique that optimizes input images to maximize specific neuron activations
- **Grad-CAM**: Gradient-weighted Class Activation Mapping, a method to visualize where a CNN focuses attention
- **Robust training**: Training strategies designed to improve model performance on distribution-shifted or adversarially perturbed data
- **Counterfactual**: A modified version of an input where a specific feature is changed to test causal effects on predictions
- **Sparse autoencoder**: A neural network trained to compress and reconstruct activations with a sparsity constraint, used for feature decomposition
- **Targeted adversarial attack**: An optimization-based method to perturb an input to be misclassified as a specific target class

---

## 12. Appendices

### Appendix A: Example color mapping for Colored-MNIST

| Digit | Dominant Color (Easy) | Hard Test Colors |
|-------|----------------------|------------------|
| 0 | Red | Any except red |
| 1 | Green | Any except green |
| 2 | Blue | Any except blue |
| 3 | Yellow | Any except yellow |
| 4 | Magenta | Any except magenta |
| 5 | Cyan | Any except cyan |
| 6 | Orange | Any except orange |
| 7 | Purple | Any except purple |
| 8 | Pink | Any except pink |
| 9 | Brown | Any except brown |

### Appendix B: Recommended hyperparameters

#### Baseline training
- Learning rate: 0.001
- Batch size: 128
- Epochs: 30
- Optimizer: Adam
- Weight decay: 1e-4

#### Robust training (example)
- Learning rate: 0.0005
- Batch size: 128
- Epochs: 50
- Optimizer: Adam
- Gradient penalty weight: 0.01
- Invariance loss weight: 0.1

#### Activation maximization
- Learning rate: 0.1
- Iterations: 1000
- L2 regularization: 0.001
- Gaussian blur interval: Every 10 steps

#### Adversarial attack
- Attack: PGD with 10 iterations
- Step size: 0.01
- Epsilon: 0.05
- Target confidence threshold: 0.9

### Appendix C: References and resources

- **Grad-CAM paper**: Selvaraju et al., "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization" (ICCV 2017)
- **Activation maximization**: Erhan et al., "Visualizing Higher-Layer Features of a Deep Network" (2009)
- **Spurious correlations**: Geirhos et al., "Shortcut Learning in Deep Neural Networks" (Nature Machine Intelligence 2020)
- **Robust training**: Sagawa et al., "Distributionally Robust Neural Networks" (ICLR 2020)
- **Sparse autoencoders**: Bricken et al., "Towards Monosemanticity" (Anthropic 2023)

</PRD>